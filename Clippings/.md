---
title: "Search | arXiv e-print repository"
source: "https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI"
author:
published:
created: 2025-02-28
description:
tags:
  - "clippings"
---
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

Authors: [DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Dejian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Haowei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Junxiao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+J), [Ruoyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Runxin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Qihao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q), [Shirong Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Peiyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Xiao Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+X), [Xiaokang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Xingkai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Z. F. Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z+F), [Zhibin Gou](https://arxiv.org/search/cs?searchtype=author&query=Gou%2C+Z), [Zhihong Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Z), [Zhuoshu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Ziyi Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z), [Aixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Bing Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+B), [Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Bochao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B), [Bei Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+B), [Chengda Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+C) , et al. (175 additional authors not shown)

Abstract: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters… ▽ More We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. △ Less

Submitted 22 January, 2025; originally announced January 2025.
- DeepSeek-V3 Technical Report

Authors: [DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI), [Aixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Bei Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+B), [Bing Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+B), [Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Bochao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B), [Chengda Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+C), [Chenggang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Chengqi Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+C), [Chenyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Chong Ruan](https://arxiv.org/search/cs?searchtype=author&query=Ruan%2C+C), [Damai Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Dejian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Deli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [Dongjie Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+D), [Erhang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E), [Fangyun Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+F), [Fucong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+F), [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Guangbo Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+G), [Guanting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Guowei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [H. Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Han Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H) , et al. (175 additional authors not shown)

Abstract: We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for loa… ▽ More We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. △ Less

Submitted 18 February, 2025; v1 submitted 26 December, 2024; originally announced December 2024.
- DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence

Authors: [DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI), [Qihao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Zhihong Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Z), [Dejian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Peiyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Runxin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Y. Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Yukun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Huazuo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Shirong Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Wangding Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W), [Xiao Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+X), [Zihui Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Z), [Hanwei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Damai Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D), [Kai Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+K), [Liyue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Yishi Piao](https://arxiv.org/search/cs?searchtype=author&query=Piao%2C+Y), [Zhibin Gou](https://arxiv.org/search/cs?searchtype=author&query=Gou%2C+Z), [Zhenda Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Z), [Zhewen Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Z), [Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Junxiao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+J), [Deli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D) , et al. (15 additional authors not shown)

Abstract: We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathe… ▽ More We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks. △ Less

Submitted 17 June, 2024; originally announced June 2024.
- DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

Authors: [DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI), [Aixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Bei Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+B), [Bin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Bo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Chenggang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Chengqi Dengr](https://arxiv.org/search/cs?searchtype=author&query=Dengr%2C+C), [Chong Ruan](https://arxiv.org/search/cs?searchtype=author&query=Ruan%2C+C), [Damai Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Dejian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Deli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [Dongjie Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+D), [Erhang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E), [Fangyun Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+F), [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Guangbo Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+G), [Guanting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Guowei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [H. Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Hanwei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Hao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H), [Haowei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Honghui Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+H) , et al. (132 additional authors not shown)

Abstract: We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference… ▽ More We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. △ Less

Submitted 19 June, 2024; v1 submitted 7 May, 2024; originally announced May 2024.
- DeepSeek LLM: Scaling Open-Source Language Models with Longtermism

Authors: [DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&query=DeepSeek-AI), [:](https://arxiv.org/search/cs?searchtype=author&query=%3A), [Xiao Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+X), [Deli Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [Guanting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Shanhuang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Damai Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D), [Chengqi Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+C), [Honghui Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+H), [Kai Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+K), [Qiushi Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q), [Zhe Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z), [Huazuo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Kaige Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+K), [Wenjun Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+W), [Ruiqi Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+R), [Kang Guan](https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+K), [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Jianzhong Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Guangbo Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+G), [Zhewen Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Z), [Ying He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y), [Wenjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W), [Panpan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Erhang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E) , et al. (63 additional authors not shown)

Abstract: The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B… ▽ More The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5. △ Less

Submitted 5 January, 2024; originally announced January 2024.