---
title: "Famous Deep Learning Papers"
source: "https://papers.baulab.info/"
author:
published:
created: 2025-08-09
description: "Famous Deep Learning Papers"
tags:
  - "clippings"
---
There are thousands of deep learning papers; where to start? Here is a curated list of the greatest hits.  
深度学习论文有数千篇；从哪里开始呢？这里是一份精选的必读论文列表。

| Year 年份 | Author/Title 作者/标题 | Notes 注释 |
| --- | --- | --- |
| 1943 | [Warren S. McCulloch and Walter Pitts, *A Logical Calculus of the Ideas Immanent in Nervous Activity*   沃伦·斯·麦克洛克和沃尔特·皮茨，《神经活动中内在思想的逻辑演算》](https://papers.baulab.info/papers/McCullochPitts-1943.pdf) | Is a neural network a computing machine? McCulloch and Pitts are the first to model neural networks as an abstract computational system. They find that under various assumptions, networks of neurons are as powerful as propositional logic, sparking widespread interest in neural models of computation.   神经网络是一种计算机器吗？麦克洛克和皮茨是第一个将神经网络建模为抽象计算系统的学者。他们发现，在多种假设下，神经网络的威力与命题逻辑相当，从而引发了人们对大脑计算模型的广泛兴趣。 |
| 1958 | [Frank Rosenblatt, *The Perceptron: A Probablistic Model for Information Storage and Organization in the Brain*   弗兰克·罗森布拉特，《感知器：大脑中信息存储和组织的概率模型》](https://papers.baulab.info/papers/Rosenblatt-1958.pdf) | Can an artificial neural network learn? Rosenblatt proposes the **Perception Algorithm**, a method for iteratively adjusting variable weight connections between neurons to learn to solve a problem. He raises funds from the U.S. Navy to build a physical Perceptron machine. [In press coverage, Rosenblatt anticipates walking, talking, self-conscious machines.](https://papers.baulab.info/papers/also/NyTimes-1958.pdf)   神经网络能否学习？罗森布拉特提出了感知算法，这是一种通过迭代调整神经元之间的可变权重连接来学习解决问题的方法。他获得了美国海军的资金来建造一台物理感知器机器。在新闻报道中，罗森布拉特预测了会行走、会说话、有自我意识的机器。 |
| 1959 | [Jerome Lettvin, Humberto Maturana, Warren McCulloch and Walter Pitts, *What the Frog's Eye Tells the Frog's Brain*   杰罗姆·莱特文、胡安·马图拉纳、沃伦·麦克洛克和沃尔特·皮茨，《青蛙的眼睛告诉青蛙的大脑》](https://papers.baulab.info/papers/Lettvin-1959.pdf) | Do nerves transmit ideas? Lettvin provocatively proposes that the frog optic nerve signals the presence of meaningful patterns rather than just brightness, demonstrating that the eye is doing part of the computational work of vision. [Lettvin is also known for his famous thought experiment](https://papers.baulab.info/papers/also/Gross-2002.pdf) that your brain might contain a **Grandmother Neuron** that you use to conceptualize your grandmother.   神经是否传递思想？莱特文挑衅地提出，青蛙的视神经传递的是有意义模式的信号，而不仅仅是亮度，证明眼睛在做部分视觉计算工作。莱特文也因一个著名的思想实验而闻名，即你的大脑中可能存在一个祖母神经元，你用它来概念化你的祖母。 |
| 1959 | [David H. Hubel and Torsten N Wiesel, *Receptive Fields of Single Neurones in the Cat's Striate Cortex*   大卫·H·休伯尔和托斯顿·N·魏斯尔，《猫的纹状皮层单个神经元的感受野》](https://papers.baulab.info/papers/HubelWiesel-1959.pdf) | How does biological vision work? This paper and [its 1962 extension](https://papers.baulab.info/papers/also/HubelWiesel-1962.pdf) kick off a [25-year collaboration](https://papers.baulab.info/papers/also/Wurtz-2009.pdf) in which Hubel and Wiesel methodically analyze the processing of signals through mammalian visual systems, developing many specific insights about the operation of the **Visual Cortex** that later inspire and inform the design of convolutional neural networks. [They win the Nobel Prize in 1981](https://papers.baulab.info/papers/also/Berlucci-2006.pdf).   生物视觉是如何工作的？这篇论文及其 1962 年的扩展开启了长达 25 年的合作，Hubel 和 Wiesel 系统地分析了哺乳动物视觉系统中信号的处理过程，发展出许多关于视觉皮层运作的具体见解，这些见解后来启发了卷积神经网络的设计与构建。他们于 1981 年获得了诺贝尔奖。 |
| 1969 | [Marvin Minsky and Seymour Papert, *Perceptrons: An Introduction to Computational Geometry*   马文·明斯基和西摩·帕普特，《感知器：计算几何导论》](https://papers.baulab.info/papers/Minsky-1969.pdf) | What *cannot* be learned by a perceptrons? During the early 1960's, while [Rosenblatt argues that his neural networks could do almost anything, Minsky counters that they could do very little](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm). This influential book lays out the negative argument, showing that many simple problems such as maze-solving or even XOR cannot be solved by a single-layer perceptron network. The sharp critique leads to one of the first **AI Winter** periods during which many researchers abandon neural networks.   感知器不能学习什么？在 20 世纪 60 年代初，当罗森布拉特认为他的神经网络几乎可以完成任何事情时，明斯基反驳说它们能做的事情非常少。这本有影响力的书提出了负面论点，表明许多简单问题，如迷宫求解甚至 XOR，都无法由单层感知器网络解决。尖锐的批评导致了第一次人工智能寒冬，许多研究人员放弃了神经网络。 |
| 1972 | [Teuvo Kohonen, *Correlation Matrix Memories*   Teuvo Kohonen，《相关矩阵记忆》](https://papers.baulab.info/papers/Kohonen-1972.pdf) | Can a neural network store memories? Kohonen (and simultaneously [Anderson](https://papers.baulab.info/papers/also/Anderson-1972.pdf)) observe that a single-layer network can act as a matrix **Associative Memory** if keys and data are seen as *vectors* of neural activations, and if keys are [linearly independent](https://papers.baulab.info/papers/also/Kohonen-1973.pdf). Associative memory will become a [major focus of neural network research in coming decades](https://papers.baulab.info/papers/also/Carpenter-1991.pdf).   神经网络能否存储记忆？科霍恩（同时与安德森）观察到，如果将键和数据视为神经激活的向量，并且键线性独立，那么单层网络可以作为矩阵联想记忆。联想记忆将在未来几十年成为神经网络研究的主要焦点。 |
| 1981 | [Geoffrey E. Hinton, *Implementing Semantic Networks in Parallel Hardware*   杰弗里·E·希顿，《在并行硬件中实现语义网络》](https://papers.baulab.info/papers/Hinton-1981.pdf) | How are concepts represented? Writing in a [book on associative memory with Anderson](https://www.google.com/books/edition/Parallel_Models_of_Associative_Memory/Ug7sAgAAQBAJ), Hinton proposes that concepts should not be represented as single units, but as *vectors* of activations, and he demonstrates a scheme that encodes complex relationships in a distributed fashion. Distributed representation becomes a core tenet of the **Parallel Distributed Processing** (PDP) framework, [advanced in a subsequent book by Rumelhart, McCleland, and Hinton (1986)](https://papers.baulab.info/papers/also/Hinton-1986.pdf), and a central dogma in the understanding of large neural networks.   概念如何表示？在撰写与安德森合著的关于联想记忆的书籍中，希顿提出概念不应表示为单个单元，而应表示为激活的向量，并展示了一种分布式编码复杂关系的方案。分布式表示成为并行分布式处理（PDP）框架的核心原则，在 Rumelhart、McCleland 和 Hinton（1986）的后续著作中进一步推进，并成为理解大型神经网络的核心教条。 |
| 1986 | [David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams, *Learning Representations by Back-Propagating Errors*   大卫·E·鲁梅尔哈特、杰弗里·E·希顿和罗纳德·J·威廉姆斯，《通过反向传播错误来学习表示》](https://papers.baulab.info/papers/Rumelhart-1986.pdf) | How can a deep network learn? Learning in *multilayer* networks was not widely understood until this paper's explanation of the **Backpropagation** method, which updates weights by efficiently computing gradients. While [Griewank (2012) notes](https://papers.baulab.info/papers/also/Griewank-2012.pdf) that reverse-mode auto-differentiation was discovered independently several times, notably by [Seppo Linnainmaa (1970)](https://papers.baulab.info/papers/also/Linnainmaa-1976.pdf) and by [Paul Werbos (1981)](https://papers.baulab.info/papers/also/Werbos-1981.pdf), Rumelhart's letter to Nature demonstrating its power to learn nontrivial representations gains widespread attention and unleashes a new wave of innovation in neural networks.   深度网络如何学习？直到这篇论文解释了反向传播方法，多层网络中的学习才被广泛理解，该方法通过高效计算梯度来更新权重。虽然格里旺（2012）指出反向模式自动微分被多次独立发现，特别是由塞波·林奈马（1970）和保罗·韦尔博斯（1981）发现，但拉姆尔哈特的《自然》杂志信件展示了其学习非平凡表示的能力，获得了广泛关注，并在神经网络中掀起了一股创新浪潮。 |
| 1988 | [Sarah Solla, Esther Levin and Michael Fleisher, *Accelerated Learning in Layered Neural Networks*   莎拉·索拉、埃丝特·利文和迈克尔·弗莱施尔，《分层神经网络的加速学习》](https://papers.baulab.info/papers/Solla-1988.pdf) | What should deep networks learn? In three concurrent papers, Solla *et al*, [John Hopfield (1987)](https://papers.baulab.info/papers/also/Hopfield-1987.pdf) and [Eric Baum and Frank Wilczek (1988)](https://papers.baulab.info/papers/also/Baum-1988.pdf) describe the insight that neural networks should often compute *log probabilities* rather than just arbitrary scales of numbers and that the **Cross Entropy Objective** is frequently more natural and more effective than squared error minimization. (How effective remains an open area of research: see [Hui 2021](https://papers.baulab.info/papers/also/Hui-2021.pdf) and [Golik 2013](https://papers.baulab.info/papers/also/Golik-2013.pdf).)   深度网络应该学习什么？在三项同时发表的论文中，索拉等人、约翰·霍普菲尔德（1987）以及埃里克·鲍姆和弗兰克·维尔切克（1988）描述了神经网络应该经常计算对数概率而不是任意数值尺度的洞察，并且交叉熵目标通常比平方误差最小化更自然、更有效。（其有效性仍然是一个开放的研究领域：参见胡（2021）和戈利克（2013）。） |
| 1989 | [Yann Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and L. D. Jackel, *Handwritten Digit Recognition with a Back-Propagation Network*   Yann Le Cun、B. Boser、J. S. Denker、D. Henderson、R. E. Howard、W. Hubbard 和 L. D. Jackel，《使用反向传播网络的手写数字识别》](https://papers.baulab.info/papers/LeCun-1989.pdf) | Can a deep network learn to see? In a technical tour-de-force, Le Cun devises the **Convolutional Neural Network** [(CNN)](https://papers.baulab.info/papers/about/LeCun-2010.pdf) (inspired and informed by Hubel and Weisel's biological studies), and demonstrates that backpropagation can train a CNN to accurately read handwritten digits on U.S. Postal Mail addresses. The work demonstrates the value of a good network architecture, and proves that deep networks can solve real-world problems. Also see [Fukushima 1980](http://papers.baulab.info/papers/also/Fukushima-1980.pdf) for an early variant of this idea.   深度网络能否学会看？在技术上的壮举中，Le Cun 设计了卷积神经网络（CNN）（受 Hubel 和 Weisel 的生物学研究的启发和信息），并展示了反向传播可以训练 CNN 准确读取美国邮政邮件地址上的手写数字。这项工作展示了良好网络架构的价值，并证明了深度网络可以解决现实世界的问题。也请参见 Fukushima 1980，这是该想法的早期变体。 |
| 1989 | [George Cybenko, *Approximation by Superpositions of a Sigmoidal Function\**   George Cybenko，《Sigmoidal 函数叠加的逼近\*》](https://papers.baulab.info/papers/Cybenko-1989.pdf) | What functions can a deep network compute? This paper proves that *any* continuous function can be closely approximated by a neural network to arbitrary small error on a finite domain. Cybenko's reasoning is specific to the sigmoid nonlinearities popular at the time, but [Hornik (1991)](https://papers.baulab.info/papers/also/Hornik-1991.pdf) shows that the result can be generalized to essentially any ordinary nonlinearity, and that two layers is enough. Cybenko and Hornik's results show that networks with multiple layers are **Universal Approximators**, far more expressive than the single-layer perceptrons proposed in the 50s and 60s.   一个深度网络能计算什么函数？这篇论文证明了任何连续函数都可以在一个有限域上被神经网络紧密逼近，误差可以任意小。Cybenko 的推理针对当时流行的 Sigmoid 非线性，但 Hornik（1991）表明这个结果可以推广到本质上任何普通非线性，并且只需要两层。Cybenko 和 Hornik 的结果表明，多层网络是通用逼近器，比 50 年代和 60 年代提出的单层感知器更具表达能力。 |
| 1990 | [Jeffrey L Elman, *Finding Structure in Time*   Jeffrey L Elman，《在时间中寻找结构》](https://papers.baulab.info/papers/Elman-1990.pdf) | Can a deep network learn language? Adopting a three-layer **Recurrent Neural Network** (RNN) architecture devised by [Michael Jordan (1986)](https://papers.baulab.info/papers/also/Jordan-1986.pdf), Elman trains an RNN to model natural language text, starting from letters. Strikingly, he finds that the network learns to represent the structure of words, grammar, and elements of semantics.   深度网络能学习语言吗？采用 Michael Jordan（1986）设计的三层循环神经网络（RNN）架构，Elman 训练一个 RNN 来模拟自然语言文本，从字母开始。令人惊讶的是，他发现网络学会了表示词语结构、语法和语义元素。 |
| 1990 | [Léon Bottou and Patrick Gallinari *A Framework for the Cooperation of Learning Algorithms*   Léon Bottou 和 Patrick Gallinari，《学习算法合作框架》](https://papers.baulab.info/papers/Bottou-1990.pdf) | What is the right notation for neural network architecture? Bottou observes that the backpropagation algorithm allows an elegant graphical notation where instead of a graph of neurons, the network is written as a graph of computation *modules* that encapsulate vectorized forward and backward gradient computations. Bottou's modular idea is the basis for deep learning libraries such as **Torch** [(Collobert 2002)](https://papers.baulab.info/papers/also/Collobert-2002.pdf), **Theano** [(Bergstra 2010)](https://papers.baulab.info/papers/also/Bergstra-2010.pdf), **Caffe**, [(Jia 2014)](https://papers.baulab.info/papers/also/Jia-2014.pdf), **Tensorflow** [(Abadi 2016)](https://papers.baulab.info/papers/also/Abadi-2016.pdf), **JAX** [(Frostig 2018)](https://papers.baulab.info/papers/also/Frostig-2018.pdf), and **PyTorch** [(Paszke 2019)](https://papers.baulab.info/papers/also/Paszke-2019.pdf).   神经网络的架构应该如何表示？Bottou 观察到，反向传播算法允许一种优雅的图形表示法，其中不是用神经元图来表示网络，而是将网络写成一个计算模块图，这些模块封装了向量化前向和反向梯度计算。Bottou 的模块化思想是深度学习库，如 Torch（Collobert 2002）、Theano（Bergstra 2010）、Caffe（Jia 2014）、Tensorflow（Abadi 2016）、JAX（Frostig 2018）和 PyTorch（Paszke 2019）的基础。 |
| 1991 | [Léon Bottou, *Stochastic Gradient Learning in Neural Networks*   Léon Bottou，神经网络的随机梯度学习](https://papers.baulab.info/papers/Krogh-1991.pdf) | What optimization algorithm should be used? In his PhD thesis, Bottou proposes that previously proposed learning algorithms such as perceptrons correspond to **Stochastic Gradient Descent** (SGD), and he argues that SGD scales better than more complex higher-order optimization methods. Over the decades, Bottou is proved right, and variants of the simple SGD algorithm become the standard workhorse learning algorithm for neural networks. See [Bottou (1998)](https://papers.baulab.info/papers/also/Bottou-1998.pdf) and [Bottou (2010)](https://papers.baulab.info/papers/also/Bottou-2010.pdf) for newer discussions about SGD from Bottou, and also see [Zinkevich (2003)](https://papers.baulab.info/papers/also/Zinkevich-2003.pdf) for an elegant generalizable proof of convergence.   应该使用哪种优化算法？在博士论文中，Bottou 提出先前提出的如感知器等学习算法对应于随机梯度下降（SGD），并论证 SGD 比更复杂的更高阶优化方法扩展得更好。几十年来，Bottou 被证明是正确的，简单 SGD 算法的变体成为神经网络的标准工作学习算法。参见 Bottou (1998)和 Bottou (2010)中关于 SGD 的新讨论，以及 Zinkevich (2003)中关于收敛的优雅可推广证明。 |
| 1991 | [Anders Krogh and John A. Hertz, *A Simple Weight Decay Can Improve Generalization*   Anders Krogh 和 John A. Hertz，《简单的权重衰减可以改善泛化》](https://papers.baulab.info/papers/Krogh-1991.pdf) | How can overfitting be avoided? This paper analyzes and advocates **Weight Decay**, a simple regularizer originally proposed as **Ridge Regression** ([Hoerl, 1970](https://papers.baulab.info/papers/also/Hoerl-1970.pdf)) that imposes a penalty on the square of the weights of a model. Krogh analyzes this trick in neural networks, demonstrating generalization gains in single-layer and mulilayer networks.   如何避免过拟合？本文分析和倡导权重衰减，这是一种简单的正则化器，最初作为岭回归（Hoerl，1970）提出，对模型的权重平方施加惩罚。Krogh 在神经网络中分析了这个技巧，展示了单层和多层网络中的泛化增益。 |
| 1997 | [Sepp Hochreiter and Jürgen Schmidhuber, *Long Short-Term Memory*   Sepp Hochreiter 和 Jürgen Schmidhuber，《长短期记忆》](https://papers.baulab.info/papers/Hochreiter-1997.pdf) | How can long recurrences be stabilized? Iterating an RNN many times will invariably to lead to an explosion of gradients without special measures. This paper proposes the **Long Short-Term Memory** (LSTM) architecture, a gated but differentiable neural memory structure that can retain state over very long sequences while preventing the gradient from exploding. The LTSM architecture also inspires **Gated Recurrent Units** (GRU), a simpler alternative devised by [Cho 2014](https://papers.baulab.info/papers/also/Cho-2014.pdf)).   如何稳定长循环？多次迭代 RNN 会导致梯度爆炸，除非采取特殊措施。本文提出了长短期记忆（LSTM）架构，这是一种门控但可微分的神经记忆结构，能够在非常长的序列中保持状态，同时防止梯度爆炸。LSTM 架构也启发了门控循环单元（GRU），这是 Cho 2014 年设计的一种更简单的替代方案。 |
| 2003 | [Yoshua Bengio, Réjean Ducharme, Pascal Vincent and Christian Jauvin, *A Neural Probabilistic Language Model*   Yoshua Bengio, Réjean Ducharme, Pascal Vincent 和 Christian Jauvin，《神经概率语言模型》](https://papers.baulab.info/papers/Bengio-2003.pdf) | Can a neural network model language at scale? In this work, Bengio's team scales a nonrecurrent neural language model to a 15-million word training set, beating the state-of-the-art traditional language modeling methods by a large margin. Rather than using a fully recurrent network, Bengio processes a fixed window of n words and devotes a network layer to learn a position-indepenent **Word Embedding**.   神经网络能否大规模建模语言？在这项工作中，Bengio 的团队将一个非循环神经语言模型扩展到 1500 万个词的训练集中，大幅超越了当时最先进的传统语言建模方法。Bengio 没有使用全循环网络，而是处理固定窗口的 n 个词，并分配一个网络层来学习位置无关的词嵌入。 |
| 2005 | [Rodrigo Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch and Itzhak Fried, *Invariant Visual Representation by Single Neurons in the Human Brain*   Rodrigo Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch 和 Itzhak Fried，《人类大脑中单神经元的不变视觉表征》](https://papers.baulab.info/papers/Quiroga-2005.pdf) | What do individual biological neurons do? In a series of remarkable experiments probing single neurons of human epilepsy patients, several **Multimodal Neurons** are found: individual neurons that are selectively responsive to very different stimuli that evoke the same concept, for example a neuron reponsive to a written name, sketch, photo, or costumed figure of Halle Berry, while not responding to other people, suggesting a simple physical encoding for high-level concepts in the brain.   单个生物神经元的作用是什么？在一系列关于癫痫患者单个神经元的杰出实验中，发现了几种多模态神经元：这些神经元对能引发相同概念的非常不同的刺激具有选择性响应，例如一个神经元对书写的名字、素描、照片或哈莉·贝瑞的着装形象有响应，而对其他人没有响应，这表明大脑中高级概念存在简单的物理编码。 |
| 2005 | [Geoffrey Hinton, *What Kind of Graphical Model is the Brain?*   杰弗里·辛顿，《大脑是什么样的图模型？》](https://papers.baulab.info/papers/Hinton-2005.pdf) | Can networks be deepened like a spin glass? In the early 2000s, neural network research is focused on the problem of scaling networks deeper than three layers. A breakthrough comes from bidirectional-link models of neural networks inspired by [spin-glass](https://en.wikipedia.org/wiki/Spin_glass) physics, like **Hopfield Networks** [(Hopfield, 1982)](https://papers.baulab.info/papers/also/Hopfield-1982.pdf), and **Restricted Boltzmann Machines** (RBM) [(Hinton, 1983)](https://papers.baulab.info/papers/also/Hinton-1983.pdf). In 2005, Hinton shows that a RBM called a **Deep Belief Network** can train a stack of many layers efficiently, and in [2006, Hinton and Salakhutdinov](https://papers.baulab.info/papers/Hinton-2006.pdf) show that layers of autoencoders can be stacked if initialized by RBMs.   网络能否像自旋玻璃一样加深？在 21 世纪初，神经网络研究专注于扩展网络层数超过三层的难题。突破来自于受自旋玻璃物理学启发的双向链接神经网络模型，如霍普菲尔德网络（霍普菲尔德，1982 年）和限制玻尔兹曼机（RBM）（辛顿，1983 年）。2005 年，辛顿展示了一种称为深度信念网络的 RBM 可以高效地训练多层堆栈，2006 年，辛顿和萨拉赫特丁诺夫展示了如果由 RBM 初始化，自编码器的层可以堆叠。 |
| 2010 | [Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio and Pierre-Antoine Manzagol, *Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion*   帕斯卡尔·文森特、雨果·拉罗什埃尔、伊莎贝尔·拉乔伊、约书亚·本吉奥和皮埃尔-安托万·马纳扎尔，堆叠降噪自编码器：在具有局部降噪标准的深度网络中学习有用表示](https://papers.baulab.info/papers/Vincent-2010.pdf) | Can networks be deepend with unsupservised training? The search for simpler deep network initialization methods continues, and in 2010, Vincent finds an alternative to initialization by Boltzmann machines: train each layer as a **Denoising Autoencoder** that must learn to remove noise added to training data. That group also devises the **Contractive Autoencoder** ([Rifai, 2011](https://papers.baulab.info/papers/also/Rifai-2011.pdf)), in which a gradient penalty is incorporated into the loss.   网络能否通过无监督训练来加深？对更简单的深度网络初始化方法的探索仍在继续，2010 年，文森特找到了一个替代玻尔兹曼机初始化的方法：将每一层训练成一个必须学习去除训练数据中添加噪声的降噪自编码器。该团队还发明了收缩自编码器（Rifai，2011），其中将梯度惩罚纳入损失函数。 |
| 2010 | [Xavier Glorot and Yoshua Bengio, *Understanding the Difficulty of Training Deep Feedforward Neural Networks*   Xavier Glorot 和 Yoshua Bengio，《理解深度前馈神经网络的训练难度》](https://papers.baulab.info/papers/Glorot-2010.pdf) | Can networks be deepend with simple changes? Glorot analyzes the problems with ordinary feed-forward training and proposes **Xavier Initialization**, a simple random initialization that is scaled to avoid vanishing or exploding gradients. In a second important development, [Nair (2010)](https://papers.baulab.info/papers/also/Nair-2010.pdf) and [Glorot (2011)](https://papers.baulab.info/papers/also/Glorot-2011.pdf) experimentally find that **Rectified Linear Units** (ReLU) work much better than the sigmoid nonlinearities that have previously been ubiquitous. These simple-to-apply innovations eliminate the need for complex pretraining, so that deep feedforward networks can be trained directly, end-to-end, from scratch, using backpropagation.   网络能否通过简单修改来加深？Glorot 分析了普通前馈训练的问题，并提出了 Xavier 初始化，这是一种简单的随机初始化方法，通过缩放来避免梯度消失或梯度爆炸。在第二个重要进展中，Nair（2010）和 Glorot（2011）通过实验发现，修正线性单元（ReLU）比之前普遍使用的 Sigmoid 非线性函数效果要好得多。这些简单易用的创新消除了对复杂预训练的需求，使得深度前馈网络可以直接、端到端地从零开始，使用反向传播进行训练。 |
| 2011 | [Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa, *Natural Language Processing (Almost) from Scratch*   Ronan Collobert，Jason Weston，Léon Bottou，Michael Karlen，Koray Kavukcuoglu 和 Pavel Kuksa，自然语言处理（几乎）从零开始](https://papers.baulab.info/papers/Collobert-2011.pdf) | Can a neural network solve language problems? Previous work in natural language processing treats the problems of chunking, part-of-speech tagging, named entity recognition, and semantic role labeling separately. Collobert claims that a single neural network can do it all at once, using a **Multi-Task Objective** to learn a unified representation of language for all the tasks. They find that their network learns a satisfying word embedding that groups together meaningfully related words, but the performance claims are initially met with skepticism.   神经网络能否解决语言问题？以往的自然语言处理工作将分词、词性标注、命名实体识别和语义角色标注等问题分开处理。Collobert 声称，单个神经网络可以一次性完成所有这些任务，通过多任务目标学习所有任务的语言统一表示。他们发现，他们的网络学习到了一个令人满意的词嵌入，能够将语义相关的词语分组在一起，但性能方面的声明最初受到了质疑。 |
| 2012 | [Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton *ImageNet Classification with Deep Convolutional Neural Networks*](https://papers.baulab.info/papers/Krizhevsky-2012.pdf) | Can a neural network do state-of-the-art computer vision? Krizhevsy shocks the computer vision community with a deep convolutonal network that wins the annual ImageNet classification challenge ([Deng, 2009](https://papers.baulab.info/papers/also/Deng-2009.pdf)) by a large margin. Krizhevsky's **AlexNet** is a deep eight-layer 60-million parameter convolutional network that combines the latest tricks such as ReLU and **Dropout** ([Srivatsava, 2014](https://papers.baulab.info/papers/also/Srivatsava-2014.pdf) and [Hinton, 2012](https://papers.baulab.info/papers/also/Srivatsava-2014.pdf)), and it is run on a pair of consumer **Graphical Processing Units** (GPU). The superior performance on the high-profile large-scale benchmark sparks a sudden change in perspective towards neural networks in the ML community and an explosive resurgence of interest in deep network applications.   神经网络能否实现最先进的计算机视觉？Krizhevsy 用一个深度卷积神经网络震惊了计算机视觉领域，该网络以巨大优势赢得了年度 ImageNet 分类挑战（Deng，2009）。Krizhevsky 的 AlexNet 是一个包含最新技巧（如 ReLU 和 Dropout）的深度八层 6000 万参数卷积网络（Srivatsava，2014 和 Hinton，2012），它在一对消费级图形处理器（GPU）上运行。在高规格的大规模基准测试上的卓越性能引发了机器学习领域对神经网络的视角突变，以及深度网络应用的爆炸性兴趣复苏。 |
| 2012 | [Tomas Mikolov, Illya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean, *Distributed Representations of Words and Phrases and their Compositionality*   Tomas Mikolov，Illya Sutskever，Kai Chen，Greg Corrado 和 Jeffrey Dean，《词语和短语的分布式表示及其组合性》](https://papers.baulab.info/papers/Le-2012.pdf) | Does massive data beat a complex network? While excitement grows over the power of neural networks, Google researcher Mikolov finds that his simple (non-deep) skip-gram model ([Mikolov, 2012a](https://papers.baulab.info/papers/also/Mikolov-2012a.pdf)) can learn a good word embedding that outperforms other (deep) embeddings by a large margin if trained on a massive 30-billion word data set. This **Word2Vec** model exhibits **Semantic Vector Composition** for the first time. Google also trains an unsupervised model on Youtube image data ([Le, 2011](https://papers.baulab.info/papers/also/Le-2011.pdf)) using an Topographic Independent Component Analysis loss ([Hyvärinen 2009](https://papers.baulab.info/papers/also/Hyvarinen-2009.pdf)), and observes the emergence of individual neurons for human faces and cats.   大数据能否胜过复杂网络？随着神经网络力量的日益凸显，谷歌研究员 Mikolov 发现，他的简单（非深度）跳字模型（Mikolov, 2012a）如果在大规模 30 亿词数据集上训练，能够学习到一种性能远超其他（深度）嵌入的优质词嵌入。该 Word2Vec 模型首次展示了语义向量合成。谷歌还使用拓扑独立成分分析损失（Hyvärinen 2009）在 Youtube 图像数据上训练了一个无监督模型（Le, 2011），并观察到针对人脸和猫的独立神经元的出现。 |
| 2013 | [Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra and Martin Riedmiller, *Playing Atari with Deep Reinforcement Learning*   Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra 和 Martin Riedmiller，使用深度强化学习玩 Atari 游戏](https://papers.baulab.info/papers/Minh-2013.pdf) | Can a network learn to play a game from raw input? DeepMind proposes **Deep Reinforcement Learning** (DRL), applying neural networks directly to the Q-learning algorithm, and demonstrates that their **Deep Q-Network** (DQN) architecture directly predicts actions from state observations and can learn to control joystick controls well enough to learn to play several Atari games better than humans. The work inspires many other DRL methods such as **Deep Deterministic Policy Gradient** (DDPG) ([Lillicrap 2016](https://papers.baulab.info/papers/also/Lillicrap-2016.pdf)) and **Proximal Policy Optimization** (PPO) ([Shulman 2017](https://papers.baulab.info/papers/also/Shulman-2017.pdf)), and touches off development of Atari-capable RL testing environments like [OpenAI Gym](https://github.com/openai/gym).   网络能否从原始输入中学习玩游戏？DeepMind 提出了深度强化学习（DRL），将神经网络直接应用于 Q-learning 算法，并证明他们的深度 Q 网络（DQN）架构可以直接根据状态观察预测动作，能够学习到足够好的摇杆控制，以至于能够比人类更好地学习玩几个 Atari 游戏。这项工作启发了许多其他 DRL 方法，如深度确定性策略梯度（DDPG）（Lillicrap 2016）和近端策略优化（PPO）（Shulman 2017），并推动了 Atari-capable RL 测试环境如 OpenAI Gym 的开发。 |
| 2013 | [Diederik P. Kingma and Max Welling, *Auto-Encoding Variational Bayes*   Diederik P. Kingma 和 Max Welling，自动编码变分贝叶斯](https://papers.baulab.info/papers/Kingma-2013.pdf) | What should an autoencoder reconstruct? The **Variational Autoencoder** (VAE) casts the autoencoder as variational inference problem, matching distributions rather than instances, by maximizing the **Evidence Lower Bound** (ELBO) of the likelihood of the data by minimizing information in the stochastic latent, and using a **Reparameterization Trick** to train a sampling process at the bottleneck (see the [Doersch tutorial](https://papers.baulab.info/papers/also/Doersch-2021.pdf)). VAEs take their inspiration from [Hinton's 1995 Wake-sleep algorithm](https://papers.baulab.info/papers/also/Hinton-1995.pdf) which attacks the same problem of learning a continuous latent variable model. Descendants such as **Beta-VAE** ([Higgins 2017](https://papers.baulab.info/papers/also/Higgins-2017.pdf)) can learn disentangled representations, and **VQ-VAE** ([van der Oord 2017](https://papers.baulab.info/papers/also/VanderOord-2017.pdf)) can do state-of-the-art image generation.   一个自编码器应该重建什么？变分自编码器（VAE）将自编码器视为变分推理问题，通过最大化数据似然的下界（ELBO）来匹配分布而不是实例，通过最小化随机潜空间的熵，并使用重参数化技巧在瓶颈处训练采样过程（参见 Doersch 教程）。VAE 从 Hinton 1995 年的 Wake-sleep 算法中获得灵感，该算法攻击了学习连续潜变量模型的问题。例如 Beta-VAE（Higgins 2017）可以学习解耦表示，而 VQ-VAE（van der Oord 2017）可以进行最先进的图像生成。 |
| 2013 | [Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow and Rob Fergus, *Intriguing Properties of Neural Networks*   Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow and Rob Fergus](https://papers.baulab.info/papers/Szegedy-2013.pdf) | Do artificial neural networks have bugs? Using a simple optimization, Szegedy finds that it is easy to construct **Adversarial Examples**: inputs that are imperceptibly different from a natural input that fool a deep network into misclassifying an image. The observation touches off discoveries of further attacks (e.g., [Papernot 2017](https://papers.baulab.info/papers/also/Papernot-2017.pdf)), defenses ([Madry 2018](https://papers.baulab.info/papers/also/Madry-2018.pdf)) and evaluations ([Carlini 2017](https://papers.baulab.info/papers/also/Carlini-2017.pdf)).   人工神经网络有 bug 吗？通过一个简单的优化，Szegedy 发现很容易构造对抗样本：这些输入与自然输入几乎无法察觉地不同，却能欺骗深度网络错误分类图像。这一观察引发了更多攻击（例如 Papernot 2017）、防御（Madry 2018）和评估（Carlini 2017）的发现。 |
| 2014 | [Ross Girshick, Jeff Donahue, Trevor Darrell and Jitendra Malik, *Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation*   Ross Girshick, Jeff Donahue, Trevor Darrell 和 Jitendra Malik, 用于精确目标检测和语义分割的丰富特征层次结构](https://papers.baulab.info/papers/Girshick-2014.pdf) | Can a CNN locate an object in a scene? Computer vision is concerned with not just classifying, but locating and understanding the arrangments of objects in a scene. By exploiting the spatial arrangement of CNN features, Girshick's **R-CNN** (and Faster R-CNN, [Ren 2015](https://papers.baulab.info/papers/Ren-2015.pdf)) can identify not only the class of an object, but the location of an object in a scene via both bounding-box estimation and semantic segmentation.   卷积神经网络能在场景中定位物体吗？计算机视觉不仅关注分类，还关注定位和理解场景中物体的排列。通过利用卷积神经网络的空间特征排列，Girshick 的 R-CNN（以及 Faster R-CNN，Ren 2015）不仅能识别物体的类别，还能通过边界框估计和语义分割确定物体在场景中的位置。 |
| 2014 | [Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio and Jitendra Malik, *Generative Adversarial Nets*   Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio 和 Jitendra Malik, 生成对抗网络](https://papers.baulab.info/papers/Goodfellow-2014.pdf) | Can an adversarial objective be learned? A **Generative Adversarial Network** (GAN) is trained to imitate a data set by learning to synthesize examples that fool a second adversarial model simultaneously trained to distinguish real from generated data. The elegant method sparks a wave of new theoretical work as well as a new category of highly-realistic image generation methods such as **DCGAN** ([Radford 2016](https://papers.baulab.info/papers/also/Radford-2016.pdf)), **Wasserstein GAN** ([Arjovsky 2017](https://papers.baulab.info/papers/also/Arjovsky-2017.pdf)), **BigGAN** ([Brock 2019](https://papers.baulab.info/papers/also/Brock-2019.pdf)), and **StyleGAN** ([Karras 2019](https://papers.baulab.info/papers/also/Karras-2019.pdf)).   对抗性目标能否被学习？生成对抗网络（GAN）通过学习合成能够同时欺骗另一个用于区分真实与生成数据的对抗模型的样本，来模仿数据集进行训练。这种优雅的方法引发了新的理论工作浪潮，以及一类高度逼真的图像生成方法，如 DCGAN（Radford 2016）、Wasserstein GAN（Arjovsky 2017）、BigGAN（Brock 2019）和 StyleGAN（Karras 2019）。 |
| 2014 | [Jason Yosinksi, Jeff Clune, Yoshua Bengio and Hod Lipson *How Transferable are Features in Deep Neural Networks?*   杰森·约辛斯基、杰夫·克鲁恩、约书亚·本吉奥和霍德·利普森 深度神经网络中的特征可迁移性如何？](https://papers.baulab.info/papers/Yosinksi-2014.pdf) | Can network parameters be reused in another network? **Transfer Learning** takes layers of a pretrained network to initialize a network that is trained to solve a different problem. Yosinksi shows that such **Fine-Tuning** will outperform training a new network from scratch, and practioners quickly recognize that initialization with a large **Pretrained Model** (PTM) is a way to get a high-performance network using only a small amount of training data.   网络参数能否在另一个网络中复用？迁移学习将预训练网络的层用于初始化一个用于解决不同问题的网络。约辛斯基表明，这种微调将优于从头训练新网络，从业者很快认识到，使用大型预训练模型（PTM）进行初始化是仅用少量训练数据获得高性能网络的方法。 |
| 2014 | [Matthew D. Zeiler and Rob Fergus *Visualizing and Understanding Convolutional Networks*   马修·D·齐勒和罗布·费尔格斯 可视化与理解卷积网络](https://papers.baulab.info/papers/Zeiler-2014.pdf) | Can people understand deep networks? One of the critiques of deep learning is that its huge models are opaque to humans. Zeiler tackles this problem by reviewing and introducing several methods for **Deep Feature Visualization**, which depict individual signals within a network, and **Salience Mapping**, which summarize the parts of the input that most influence the outcome of the complex computation. Zeiler's goal of **Explainable AI** (XAI) is futher developed in feature optimization methods ([Olah 2017](https://distill.pub/2017/feature-visualization/)), feature dissection ([Bau 2017](https://papers.baulab.info/papers/also/Bau-2017.pdf)), and salience methods such as Grad-CAM ([Selvaraju 2016](https://papers.baulab.info/papers/also/Selvaraju-2016.pdf)) and Integrated Gradients ([Sundararajan 2017](https://papers.baulab.info/papers/also/Sundararajan-2017.pdf)).   人类能否理解深度网络？深度学习的一个批评是其庞大的模型对人类来说不透明。Zeiler 通过回顾和介绍几种深度特征可视化方法来解决这个问题，这些方法描绘了网络中的单个信号，以及显著性映射方法，后者总结了输入中对复杂计算结果影响最大的部分。Zeiler 的可解释人工智能（XAI）目标在特征优化方法（Olah 2017）、特征分解（Bau 2017）以及显著性方法（如 Grad-CAM（Selvaraju 2016）和集成梯度（Sundararajan 2017））中进一步发展。 |
| 2014 | [Ilya Sutskever, Oriol Vinyals and Quoc V. Le, *Sequence to Sequence Learning with Neural Networks*   伊利亚·苏茨凯弗、奥里奥尔·维尼亚尔斯和 Quoc V. Le，《使用神经网络的序列到序列学习》](https://papers.baulab.info/papers/Sutskever-2014.pdf) | Can a neural network translate human languages? Sutskever applies the LSTM architecture to English-to-French translation, combining an encoder phase with an autoregressive decoder phase. This demonstration of **Neural Machine Translation**; does not beat state-of-the art machine translation methods at the time, but its competitive performance establishes the feasibility of the neural approach to translation, one of the classical grand challenges of AI.   神经网络能否翻译人类语言？苏茨凯弗将 LSTM 架构应用于英法翻译，结合了编码阶段和自回归解码阶段。这一神经机器翻译的演示虽然当时没有超越最先进的机器翻译方法，但其竞争性能确立了神经网络翻译的可行性，翻译是人工智能领域的一个经典重大挑战之一。 |
| 2015 | [Dzmitry Bahdanau, KyngHyun Cho and Yoshua Bengio *Neural Machine Translation by Jointly Learning to Align and Translate*   Dzmitry Bahdanau、KyngHyun Cho 和 Yoshua Bengio 联合学习对齐和翻译的神经机器翻译](https://papers.baulab.info/papers/Bahdanau-2015.pdf) | Can a network learn its own attention? While CNNs compare adjacent pixels and RNNs examine adjacent words, sometimes the most important data dependencies are not adjacencies. Bahandau proposes a learned **Attention** model that can estimate which parts of the input are relevant to each part of the output. This innovation dramatically improves performance of neural machine translation, and the idea of using learnable attention proves effective for many kinds of data including graphs ([Veličković 2018](https://papers.baulab.info/papers/Velickovic-2018.pdf)), and images ([Zhang 2019](https://papers.baulab.info/papers/Zhang-2019.pdf)).   网络能否学习自己的注意力？虽然卷积神经网络比较相邻像素，循环神经网络检查相邻单词，但有时最重要的数据依赖关系并非相邻。Bahdanau 提出了一种可学习的注意力模型，该模型可以估计输入的哪些部分与输出的每个部分相关。这一创新显著提高了神经机器翻译的性能，而可学习注意力的思想被证明对许多类型的数据有效，包括图（Veličković 2018）和图像（Zhang 2019）。 |
| 2015 | [Diederik P. Kingma and Jimmy Lei Ba, *Adam: A Method for Stocastic Optimization*   Diederik P. Kingma 和 Jimmy Lei Ba，Adam：一种随机优化方法](https://papers.baulab.info/papers/Kingma-2015.pdf) | What learning rate should be used? The **Adam Optimizer** adaptively chooses the step size by using smaller steps for parameters in regions with more gradient variation. Combining ideas from **Momentum** ([Polyak 1964](https://papers.baulab.info/papers/also/Polyak-1964.pdf)), **Second-order optimization** ([Becker 1989](https://papers.baulab.info/papers/also/Polyak-1964.pdf)), **Adagrad** ([Duchi 2011](https://papers.baulab.info/papers/also/Duchi-2011.pdf)), **Adadelta** ([Zeiler 2012](https://papers.baulab.info/papers/also/Zeiler-2012.pdf)), and **RMSProp** ([Tieleman 2012](https://papers.baulab.info/papers/also/Tieleman-2012.pdf)), the Adam optimizer proves very effective in practice, enabling optimization of huge models with little or no manual tuning.   应该使用什么学习率？Adam 优化器通过使用较小步长来适应性地选择参数在梯度变化较大的区域中的步长。结合动量（Polyak 1964）、二阶优化（Becker 1989）、Adagrad（Duchi 2011）、Adadelta（Zeiler 2012）和 RMSProp（Tieleman 2012）的思想，Adam 优化器在实践中被证明非常有效，能够实现大型模型的优化，几乎无需手动调整。 |
| 2015 | [Sergei Ioffe and Christian Szegedy, *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*   谢尔盖·伊奥菲和克里斯蒂安·塞格迪，《批量归一化：通过减少内部协变量偏移来加速深度网络训练》](https://papers.baulab.info/papers/Ioffe-2015.pdf) | How can training gradients be stabilized? Even with clever initalization, in very deep ReLU networks eventually signals will get very large or very small. **Batch Normalization** solves this problem by normalizing each neuron to have zero mean and unit variance within every training batch. This practical step yields huge benefits, improving training speed, network performance and stability, and enabling very large models to be trained.   如何稳定训练梯度？即使有巧妙的初始化，在非常深的 ReLU 网络中，最终信号会变得非常大或非常小。批量归一化通过在每个训练批次中对每个神经元进行归一化，使其具有零均值和单位方差，从而解决了这个问题。这一实用步骤带来了巨大的好处，提高了训练速度、网络性能和稳定性，并使得非常大的模型能够被训练。 |
| 2015 | [Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun, *Deep Residual Learning for Image Recognition*   何恺明、张翔宇、任少卿和孙剑，《用于图像识别的深度残差学习》](https://papers.baulab.info/papers/He-2015.pdf) | Can backpropagation succeed if there are a huge number of network layers? Analyzing the propagation of gradients, He proposes the **Residual Network** (ResNet) architecture in which layers compute a vector to add to the signal, rather than transforming the signal at each layer. He also proposes **Kaiming Initialization**, a variant of Xavier initialization that takes into account nonlinearities. Together with batchnorm, these methods solve the depth problem, allowing networks to achieve state-of-the-art results with more than 100 layers.   如果网络层数量巨大，反向传播能否成功？通过分析梯度传播，他提出了残差网络（ResNet）架构，其中各层计算一个向量来添加到信号中，而不是在每个层上转换信号。他还提出了 Kaiming 初始化，这是一种考虑非线性变化的 Xavier 初始化变体。结合批量归一化，这些方法解决了深度问题，使网络能够在超过 100 层的情况下实现最先进的结果。 |
| 2015 | [Oriol Vinyals, Alexander Toshev, Samy Bengio and Dumitru Erhan, *Show and Tell: A Neural Image Caption Generator*   Oriol Vinyals、Alexander Toshev、Samy Bengio 和 Dumitru Erhan，《展示与讲述：一个神经图像描述生成器》](https://papers.baulab.info/papers/Vinyals-2015.pdf) | Can language and vision be related? This paper demonstrates that, despite the apprent disparities between modalities, neural representations for images and text can be directly connected. By simply attaching a vision network (a CNN) to a language network (an RNN), Vinyals demonstrates a system that can perform **Image Captioning**, generating accurate captions for a wide range of subjects after training on the MSCOCO dataset.   语言和视觉能否相关联？本文展示了尽管两种模态之间存在明显差异，图像和文本的神经表示可以直接连接。通过简单地将一个视觉网络（CNN）附加到一个语言网络（RNN）上，Vinyals 展示了一个能够执行图像描述的系统，该系统在 MSCOCO 数据集上训练后，能够为各种主题生成准确的描述。 |
| 2015 | [Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan and Surya Ganguli, *Deep Unsupervised Learning using Nonequilibrium Thermodynamics*](https://papers.baulab.info/papers/Sohl-Dickstein-2015.pdf) | Can a network learn by reversing the physics of diffusion? Inspired by Kingma's VAEs and Hinton's wake-sleep method as well as the dynamics of diffusion, Jascha Sohl-Dicksten proposes **Diffusion Models**, a latent variable framework which transforms Gaussian noise into a meaningful distribution iteratively by learning to reverse a diffusion process in many small steps. Later this method is extended by [Jonathan Ho (2020)](https://papers.baulab.info/papers/also/Ho-2020.pdf) to synthesize remarkably high quality images, superior to GANs, and that demonstration kicks off a wave of interest in using diffusion for image synthesis. See the tutorial paper from [Calvin Luo (2022)](https://papers.baulab.info/papers/also/Luo-2022.pdf) for detailed discussion. |
| 2016 | [David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel and Demis Hassabis, *Mastering the Game of Go with Deep Neural Networks and Tree Search*](https://papers.baulab.info/papers/Silver-2016.pdf) | Game-playing is one of the original domains used to demonstrate artificial intelligence capabilities. Yet while chess is conquered using traditional search methods in 1997, the game of Go is considered a far more subtle game, intuititive and impenetrable to brute-force computation. In this work by DeepMind, the **AlphaGo** system combines a CNN with traditional search methods to add the needed intuition, through a powerful learned board evaluation function, trained through self-play. The system achieves master-level play and bests the top-ranked human Go player Lee Sedol in a five-game match.   下棋是展示人工智能能力的原始领域之一。然而，尽管在 1997 年传统搜索方法攻克了国际象棋，围棋被认为是一种更为微妙、直观且难以通过暴力计算攻克的博弈。在这项 DeepMind 的工作中，AlphaGo 系统结合了 CNN 与传统搜索方法，通过一个强大的学习棋盘评估函数来添加所需的直觉，该函数通过自我对弈进行训练。该系统达到了大师级水平，并在五局比赛中击败了顶尖人类围棋选手李世石。 |
| 2017 | [Ashsh Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin *Attention is All You Need*   Ashsh Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin 注意力机制：你所需要的一切](https://papers.baulab.info/papers/Vaswani-2017.pdf) | While applying attention ideas of Bahdanau to acheive state-of-the art machine translation results, Vaswani discovers that the various mechanisms for supporting recurrent networks are unnecessary and can be replaced by attention. The resulting architecture, the **Transformer Network**, proves to be a scalable and versatile way of dealing with sequence data, leading to popular archtiectures such as BERT, GPT, and T5.   在应用 Bahdanau 的注意力机制以实现最先进的机器翻译结果时，Vaswani 发现支持循环网络的各种机制是不必要的，并且可以被注意力机制取代。由此产生的架构——Transformer 网络，证明是一种处理序列数据的可扩展且通用的方法，从而催生了 BERT、GPT 和 T5 等流行架构。 |
| 2017 | [Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros, *Image-to-Image Translation with Conditional Adversarial Networks*   菲利普·伊索拉、朱俊彦、周天慧和亚历克谢·A·埃夫罗斯，《条件对抗网络图像到图像的转换》](https://papers.baulab.info/papers/Isola-2017.pdf) | A wide class of image processing methods can be framed as a transformation from one image to another. In this work, Isola demonstrates that a single **Pix2Pix** architecture can be used across the problems of segmentation, image restyling, and sketch-guided image generation, by applying a GAN adversarial network to train the generative network to create realistic images that match the target domain. While Pix2Pix relies on a paired dataset of before- and after- images, it inspires **CycleGAN** ([Zhu, 2017](https://papers.baulab.info/papers/also/Zhu-2017.pdf)), which is able to learn to transform images based on data that is not explicitly paired.   许多图像处理方法都可以被理解为从一张图像到另一张图像的转换。在这项工作中，伊索拉展示了单个 Pix2Pix 架构可以通过应用 GAN 对抗网络来训练生成网络，以创建与目标域匹配的逼真图像，从而应用于分割、图像重绘和素描引导的图像生成等问题。虽然 Pix2Pix 依赖于成对的图像数据集（前图和后图），但它启发了 CycleGAN（朱，2017），后者能够基于未明确配对的数据学习图像转换。 |
| 2018 | [Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever, *Language Models are Unsupervised Multitask Learners*   亚历克·拉德福德、杰弗里·吴、雷文·查尔德、大卫·刘安、达里奥·阿莫迪和伊利亚·苏茨凯弗，《语言模型是无监督的多任务学习器》](https://papers.baulab.info/papers/Radford-2018.pdf) | Can a network learn to write simply by reading? While the original transformer architecture required paired language translation text in order to train, Radford discards the encoder portion of the network to obtain a simple autoregressive language model that can be trained on the simple task of predicting the next word in text. The resulting model, **GPT**, can be scaled to be trained on massive amounts of text, and the model and its scaled-up successors GPT-2 and GPT-3 exhibit emergent behavior such as the ability to solve a variety of tasks simply by **Prompting** the model with a natural-language request to answer a particular kind of question. These **Large Language Models (LLMs)** from the basis for a succession of AI advances in coming years.   网络能否仅通过阅读来学习写作？虽然最初的 Transformer 架构需要成对的翻译文本进行训练，但拉德福德摒弃了网络的编码器部分，以获得一个简单的自回归语言模型，该模型可以在预测文本中下一个词的简单任务上进行训练。由此产生的模型 GPT 可以扩展到在大量文本上进行训练，并且该模型及其扩展后的继任者 GPT-2 和 GPT-3 表现出涌现行为，例如仅通过用自然语言请求提示模型来回答特定类型问题的能力即可解决各种任务。这些大型语言模型（LLMs）是未来一系列人工智能进步的基础。 |
| 2019 | [Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova, *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*   雅各布·德夫林、明伟·张、肯顿·李和克里斯蒂娜·图塔诺娃，《BERT：用于语言理解的深度双向 Transformer 的预训练》](https://papers.baulab.info/papers/Devlin-2019.pdf) | Is there a universal encoding for language? While the traditional approach is to design a custom network for solving particular langauge problems, this paper proposes a **BERT** architecture that learns to encode text in universal way. BERT is trained on a denoising task, learning to fill in missing words in text, and also learning to distinguish adjacent sentence pairs from unrelated sentence pairs. This unsupervised training scheme allows BERT to be scaled up and trained on a huge amount of text. BERT makes it straightfoward to create high-quality language processing models for specialized tasks with only a small amount of data, by starting with a pretrained BERT and fine-tuning it for the task.   语言是否存在通用编码方式？传统方法是为解决特定语言问题而设计定制网络，而本文提出了一种 BERT 架构，学习以通用方式对文本进行编码。BERT 在去噪任务上进行训练，学习填补文本中缺失的词语，并学习区分相邻句子对与无关句子对。这种无监督训练方案使 BERT 能够扩展并使用大量文本进行训练。通过以预训练的 BERT 开始并针对任务进行微调，BERT 使得仅用少量数据即可轻松创建针对特定任务的高质量语言处理模型。 |
| 2020 | [Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi and Ren Ng, *NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis*   本·米尔登霍尔、普拉图尔·P·辛里瓦桑、马修·坦西克、乔纳森·T·巴隆、拉维·拉马莫尔蒂和任 Ng，NeRF：将场景表示为神经辐射场以进行视图合成](https://papers.baulab.info/papers/Mildenhall-2020.pdf) | Can a neural network model the physics of light transport? While most neural models are inspired by functions of the human brain, neural networks can be applied to learn functions in other domains. In this work, Mildenhall demonstrates **Neural Radiance Fields** (NeRF), a use of neural networks to learn to compute the full light transport within a 3d scene, by following physical rules while learning to match the light observed in a handful of photographs. By modeling the amount of light at every location and direction in a volume, a NeRF model is able to solve difficult rendering problems such as depicting a photographed scene from a new viewpoint, or showing a scene with a new object added.   神经网络能否模拟光的传输物理？虽然大多数神经网络模型受人类大脑功能的启发，但神经网络可以应用于学习其他领域中的函数。在这项工作中，米尔德霍尔展示了神经辐射场（NeRF），这是一种利用神经网络学习计算三维场景中完整光传输的方法，通过遵循物理规则来学习匹配少量照片中观察到的光线。通过模拟体积中每个位置和方向的光量，NeRF 模型能够解决诸如从新视角描绘拍摄场景或展示添加了新物体的场景等复杂的渲染问题。 |
| 2020 | [Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu and Demis Hassabis, *Improved protein structure prediction using potentials from deep learning*   安德鲁·W·塞纳尔、理查德·埃文斯、约翰·朱默珀、詹姆斯·柯克帕特里克、洛朗·西弗雷、蒂姆·格林、钟立琴、奥古斯丁·齐德克、亚历山大·W·R·尼尔森、亚历克斯·布里奇兰德、雨果·佩内多内斯、斯蒂格·彼得森、卡伦·西蒙尼亚、史蒂夫·克罗斯安、普什梅特·科利、大卫·T·琼斯、大卫·西尔弗、科拉伊·卡夫库丘格鲁和德米斯·哈萨比，使用深度学习势能改进蛋白质结构预测](https://papers.baulab.info/papers/Senior-2020.pdf) | Can a neural network model the physics of protein structure? One of the grand challenges of computational chemistry is to predict the 3d stucture of a protein from its amino acid sequence, because that structure is critical for understanding a protein's function. By training a convolutional neural network to predict residue distances on the 150,000 known protein conformations in the public Protein Data Bank, **AlphaFold** from DeepMind dramatically improves upon the state-of-the-art in protein structure prediction; the neural approach is combined with other chemistry algorithms to create full 3d predictions. The team applies their methods on all 200 million proteins in the UniProt database, contributing high-confidence predictions for essentially every protein known to biologists across a range of organisms, transforming the field of molecular biology. Together with AlphaFold 2 ([Jumper, et al. 2021](https://papers.baulab.info/papers/also/Jumper-2021.pdf)) and AlphaFold 3 ([Abramson, et al. 2024](https://papers.baulab.info/papers/also/Abramson-2024.pdf)) the work is awarded the Nobel Prize in Chemistry in 2024.   神经网络能否模拟蛋白质结构的物理特性？计算化学领域的一项重大挑战是从氨基酸序列预测蛋白质的三维结构，因为这种结构对于理解蛋白质的功能至关重要。通过训练卷积神经网络来预测公共蛋白质数据库中 15 万个已知蛋白质构象上的残基距离，DeepMind 的 AlphaFold 极大地改进了蛋白质结构预测的当前技术水平；神经网络方法与其他化学算法相结合，创建了完整的三维预测。该团队将他们的方法应用于 UniProt 数据库中的所有 2 亿种蛋白质，为生物学家所知的各种生物体中的几乎所有蛋白质提供了高置信度的预测，从而改变了分子生物学领域。随着 AlphaFold 2（Jumper 等人，2021 年）和 AlphaFold 3（Abramson 等人，2024 年）的问世，这项工作在 2024 年获得了诺贝尔化学奖。 |
| 2021 | [Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger and Ilya Sutskever, *Learning Transferable Visual Models From Natural Language Supervision*](https://papers.baulab.info/papers/Radford-2021.pdf) | Will the best image training data always be manually labeled? While huge text models such as BERT and GPT are trained without manual labels, the best training data in vision is still laboriously manually labeled. This work changes the situation, demonstrating an image representation supervised by automatically scraped open-text image caption data from the internet. **CLIP** applies **Contrastive Learning** on a massive 400 million captioned-image data set to jointly learn aligned image and text encodings, approaching state-of-the-art classification on a **zero-shot** test without any fine-tuning. CLIP establishes a new state-of-the-art image representation and is also an essential part of OpenAI's [DALL-E text-to-image synthesis system](https://papers.baulab.info/papers/also/Ramesh-2021.pdf). |
| 2022 | [Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser and Björn Ommer. *High-Resolution Image Synthesis With Latent Diffusion Models*](https://papers.baulab.info/papers/Rombach-2022.pdf) | Can a neural net learn to draw? A set of 2022 papers mark a remarkable advance in the state-of-the-art in text-to-image synthesis. By applying diffusion models together with text supervision using CLIP representations,[DALL-E 2 (Ramesh, et al. 2022)](https://papers.baulab.info/papers/also/Ramesh-2022.pdf) demonstrates an uncanny ability to create images from a text description that are obviously novel yet also realistic compositions of real ideas. Then by incorporating improved text conditioning from [clasifier-free guidance (Ho and Salimans 2022)](https://papers.baulab.info/papers/also/Ho-2022.pdf), stacking diffusion onto an efficient VAE image representation, and then training the mdel on [LAION (Schuhmann, et al. 2022)](https://papers.baulab.info/papers/also/Schuhmann-2022.pdf), **Latent Diffusion** (the architecture for the open pretrained **Stable Diffusion** model) sparks the rapid development of several commercial as well as open-source projects for text-guided image synthesis that are practical for widespread deployment. The work raises new societal questions about the use of AI in art as well as misinformation. |
| 2022 | [Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike and, Ryan Lowe. *Training language models to follow instructions with human feedback*](https://papers.baulab.info/papers/Ouyang-2022.pdf) | Can a neural net converse like a human? Since [Alan Turing's 1950 **imitation game**](https://papers.baulab.info/papers/also/Turing-1950.pdf), human dialog has been a benchmark for artificial intelligence. After Google's **FLAN** project ([Wei, et al. 2022)](https://papers.baulab.info/papers/also/Wei-2022.pdf) observes surprising generalization in an LLM fine-tuned to follow natural language, the OpenAI AI Safety team develops a scalable approach to fine-tuning by applying **RLHF**, Reinforcement Learning from Human Feedback, ([Christiano, et al. 2017)](https://papers.baulab.info/papers/also/Christiano-2017.pdf) to train an LLM to conform to human preferences during dialog. The resulting product, released as **ChatGPT**, smashes the Turing test with flying colors and transforms the world's perception of AI.[Wei, et al. (2022b)](https://papers.baulab.info/papers/also/Wei-2022b.pdf) observe that **Chain-of-Thought** prompting strengthens reasoning further, and Microsoft researchers [Bubek, et al (2023)](https://papers.baulab.info/papers/also/Bubek-2023.pdf) suggest that the system shows "Sparks of **AGI** ", Artificial *General* Intelligence, i.e., human-level reasoning on a broad range of tasks. ChatGPT inspires a rush of commercial competitors, and variations on RLHF are proposed including [Constitutional RL (Bai 2022)](https://papers.baulab.info/papers/also/Bai-2022.pdf), which uses LLMs to check consistency with human instructions, and [**DPO**, Direct Preference Optimization (Rafailov, et al 2023)](https://papers.baulab.info/papers/also/Rafailov-2023.pdf) which proposes a much simpler training objective for fine-tuning LLMs. |
| 2025 | [DeepSeek AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and over 100 additional auhors. *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*](https://papers.baulab.info/papers/DeepSeek-2025.pdf) | Can a neural net reason? By 2025 a vast amount of capital has poured into scaling up LLM training, with several companies competing to push benchmarks higher, and LLM training hits two barriers. First is the limit of imitation learning, after large-scale training has already incorporated most of the high-quality human-created data in the world. The second is the inability of a transformer LM to reason beyond the finite steps allowed by the fixed depth of the architecture.**Reasoning Models** address both problems by introducing deep reinforcement learning objectives into LLM training that incentivize the LM to generate an internal "chain-of-thought" monologue that are not copies of the training data. Hiding its methods behind a veil of secrecy,[OpenAI releases GPT4-O1](https://papers.baulab.info/papers/also/OpenAI-2024.pdf), the first reasoning LLM. Then one month later, [DeepSeek releases DeepSeek-R1 openly, publishing many key details about the training methods while also openly releasing its weights](https://papers.baulab.info/papers/DeepSeek-2025.pdf). DeepSeek is trained using **GRPO** ([Shao, 2024](https://papers.baulab.info/papers/also/Shao-2024.pdf)), an RL training method inspired by PPO that eliminates the reward model. |
